{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJQEuwTHMXSx"
      },
      "source": [
        "# 1: Install Required Packages\n",
        "\n",
        "Since seaborn is used for the confusion matrix and isn't installed by default in Colab, we'll install it. Other required libraries like torch, matplotlib, numpy, and pillow (for PIL) are typically pre-installed, but we'll ensure seaborn is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVNqTS7rMZyi",
        "outputId": "f6324661-e1ae-406a-c84c-c1a213b40de2"
      },
      "outputs": [],
      "source": [
        "%pip install seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2FpKc3gMd6u"
      },
      "source": [
        "# 2: Import Libraries\n",
        "\n",
        "Here, we'll import all necessary libraries, removing tkinter since it won't be used and adding ipywidgets for the new interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oDK59w40tN-e"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransforms\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torchvision\\models\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01malexnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconvnext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdensenet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mefficientnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torchvision\\models\\convnext.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstochastic_depth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_presets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torchvision\\ops\\__init__.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgiou_loss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generalized_box_iou_loss\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpoolers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiScaleRoIAlign\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mps_roi_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ps_roi_align, PSRoIAlign\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mps_roi_pool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ps_roi_pool, PSRoIPool\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torchvision\\ops\\poolers.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mboxes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m box_area\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mroi_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roi_align\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# is not supported by ONNX tracing yet.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# that merges the levels to the right indices\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;129m@torch\u001b[39m.jit.unused\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_onnx_merge_levels\u001b[39m(levels: Tensor, unmerged_results: List[Tensor]) -> Tensor:\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torchvision\\ops\\roi_align.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_compile_supported\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mannotations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BroadcastingList2\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pair\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, exc, logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging, trace_rules, variables\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\_dynamo\\exc.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompileId\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\_dynamo\\utils.py:65\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal, TypeIs\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\_functorch\\config.py:66\u001b[39m\n\u001b[32m     47\u001b[39m enable_remote_autograd_cache = remote_autograd_cache_default()\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# When AOTAutograd regenerates aliased graph outputs,\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# attempt to use functionalization's view-replay logic\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# before falling back to the autograd engine's view replay or as_strided.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# once XLA pin update works,\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# or default config to true and fix relevant bugs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fbcode\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# View replay is currently not compatible with AOTAutogradCache, since\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# FunctionalTensors are not serializable. We'll need to make them\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# serializable before enabling warm cache with this config turned on.\u001b[39;00m\n\u001b[32m     72\u001b[39m view_replay_for_aliased_outputs = (\u001b[38;5;129;01mnot\u001b[39;00m is_fbcode()) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m enable_autograd_cache)\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\_inductor\\__init__.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, TYPE_CHECKING, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\python-projects\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\_inductor\\config.py:161\u001b[39m\n\u001b[32m    154\u001b[39m b2b_gemm_pass = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# register custom graph optimization pass hook. so far, pre/post passes are\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# only applied before/after pattern_matcher in post_grad_passes.\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Implement CustomGraphPass to allow Inductor to graph compiled artifacts\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# to which your custom passes have been applied:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m post_grad_custom_pre_pass: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_inductor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_graph_pass\u001b[49m.CustomGraphPassType = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    162\u001b[39m post_grad_custom_post_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Registers a custom joint graph pass.\u001b[39;00m\n",
            "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import widgets, Layout\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "import time\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaoVvXjqMnMM"
      },
      "source": [
        "# 3: Define the ImprovedCNN Class\n",
        "\n",
        "The ImprovedCNN class remains unchanged as it defines the model architecture and doesn't rely on Tkinter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XOmDmYvLtWo2"
      },
      "outputs": [],
      "source": [
        "class ImprovedCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    An improved CNN architecture for digit recognition with:\n",
        "    - Batch normalization\n",
        "    - Residual connections\n",
        "    - Increased channel depth\n",
        "    - Global average pooling\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "\n",
        "        # Initial convolution block\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Residual block 1\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Residual block 2\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Global average pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.bn6 = nn.BatchNorm1d(64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial convolution block\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Residual block 1\n",
        "        identity = x\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        if identity.size(1) != x.size(1):\n",
        "            identity = F.pad(identity, (0, 0, 0, 0, 0, x.size(1) - identity.size(1)))\n",
        "        x = F.relu(x + identity)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Residual block 2\n",
        "        identity = x\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.bn5(self.conv5(x))\n",
        "        if identity.size(1) != x.size(1):\n",
        "            identity = F.pad(identity, (0, 0, 0, 0, 0, x.size(1) - identity.size(1)))\n",
        "        x = F.relu(x + identity)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = self.gap(x)\n",
        "        x = x.view(-1, 128)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.bn6(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceuea3e8Mqoh"
      },
      "source": [
        "# 4: Define the DigitRecognizer Class\n",
        "\n",
        "We'll modify the DigitRecognizer class by removing the create_drawing_interface method (which uses Tkinter) and adding a new create_colab_drawing_interface method for the Colab-compatible interface. The rest of the class remains largely the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6P5cMVMstlU-"
      },
      "outputs": [],
      "source": [
        "class DigitRecognizer:\n",
        "    def __init__(self, model_path=None):\n",
        "        \"\"\"\n",
        "        Initialize the digit recognizer with device setup, data preparation, and model loading.\n",
        "        Allows loading a pre-trained model if model_path is provided.\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "        self.test_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            self.load_model(model_path)\n",
        "\n",
        "    def create_model(self):\n",
        "        \"\"\"Create and return a CNN model with modern architecture enhancements.\"\"\"\n",
        "        model = ImprovedCNN().to(self.device)\n",
        "        return model\n",
        "\n",
        "    def prepare_data(self, batch_size=128):\n",
        "        \"\"\"Load and prepare MNIST dataset with data augmentation.\"\"\"\n",
        "        train_data = datasets.MNIST(\n",
        "            root=\"data\",\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=self.train_transform\n",
        "        )\n",
        "\n",
        "        test_data = datasets.MNIST(\n",
        "            root=\"data\",\n",
        "            train=False,\n",
        "            download=True,\n",
        "            transform=self.test_transform\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_data,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2,  # Reduced workers for Colab compatibility\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_data,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        self.loaders = {'train': train_loader, 'test': test_loader}\n",
        "        self.test_data = test_data\n",
        "        return self.loaders\n",
        "\n",
        "    def train_model(self, epochs=10, lr=0.001, weight_decay=1e-5):\n",
        "        \"\"\"Train the model with advanced techniques.\"\"\"\n",
        "        if not hasattr(self, 'loaders'):\n",
        "            self.prepare_data()\n",
        "\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_accuracy = 0\n",
        "        patience = 3\n",
        "        patience_counter = 0\n",
        "        train_losses = []\n",
        "        test_accuracies = []\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            epoch_start = time.time()\n",
        "\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for batch_idx, (data, target) in enumerate(self.loaders['train']):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                output = self.model(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "\n",
        "                if batch_idx % 50 == 0:\n",
        "                    print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(self.loaders['train'].dataset)} \"\n",
        "                          f\"({100. * batch_idx / len(self.loaders['train']):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "\n",
        "            avg_loss = total_loss / len(self.loaders['train'])\n",
        "            train_accuracy = 100. * correct / total\n",
        "            train_losses.append(avg_loss)\n",
        "\n",
        "            test_accuracy = self.evaluate_model()\n",
        "            test_accuracies.append(test_accuracy)\n",
        "\n",
        "            scheduler.step(avg_loss)\n",
        "\n",
        "            if test_accuracy > best_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                patience_counter = 0\n",
        "                self.save_model(best=True)\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs!\")\n",
        "                    break\n",
        "\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            print(f\"Epoch {epoch} completed in {epoch_time:.2f}s. Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Training completed in {total_time:.2f} seconds.\")\n",
        "        print(f\"Best test accuracy: {best_accuracy:.2f}%\")\n",
        "\n",
        "        self.save_model()\n",
        "        self.visualize_training(train_losses, test_accuracies)\n",
        "        self.generate_confusion_matrix()\n",
        "\n",
        "        return train_losses, test_accuracies\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        \"\"\"Evaluate the model on the test dataset.\"\"\"\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in self.loaders['test']:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "\n",
        "        accuracy = 100. * correct / total\n",
        "        print(f\"Test set: Accuracy: {correct}/{total} ({accuracy:.2f}%)\")\n",
        "        return accuracy\n",
        "\n",
        "    def save_model(self, best=False):\n",
        "        \"\"\"Save the model with metadata.\"\"\"\n",
        "        directory = \"saved_models\"\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"digit_model_best.pth\" if best else f\"digit_model_{timestamp}.pth\"\n",
        "        path = os.path.join(directory, filename)\n",
        "\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'model_architecture': 'ImprovedCNN',\n",
        "            'timestamp': timestamp\n",
        "        }, path)\n",
        "\n",
        "        print(f\"Model saved to {path}\")\n",
        "        return path\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load a saved model.\"\"\"\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"Model file not found: {model_path}\")\n",
        "            return False\n",
        "\n",
        "        checkpoint = torch.load(model_path, map_location=self.device)\n",
        "        if checkpoint.get('model_architecture') == 'ImprovedCNN':\n",
        "            self.model = ImprovedCNN().to(self.device)\n",
        "        else:\n",
        "            print(\"Warning: Unknown model architecture, using default ImprovedCNN\")\n",
        "            self.model = ImprovedCNN().to(self.device)\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"Model loaded from {model_path}\")\n",
        "        return True\n",
        "\n",
        "    def visualize_training(self, train_losses, test_accuracies):\n",
        "        \"\"\"Visualize training progress.\"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        # Create the 'plots' directory if it doesn't exist\n",
        "        os.makedirs(\"plots\", exist_ok=True)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='Training Loss')\n",
        "        plt.title('Training Loss Over Epochs')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(test_accuracies, label='Test Accuracy')\n",
        "        plt.title('Test Accuracy Over Epochs')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"plots/training_progress.png\")\n",
        "        plt.show()\n",
        "\n",
        "    def generate_confusion_matrix(self):\n",
        "        \"\"\"Generate and visualize confusion matrix.\"\"\"\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in self.loaders['test']:\n",
        "                data = data.to(self.device)\n",
        "                output = self.model(data)\n",
        "                pred = output.argmax(dim=1, keepdim=True).cpu()\n",
        "                all_preds.extend(pred.numpy().flatten())\n",
        "                all_targets.extend(target.numpy())\n",
        "\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.savefig(\"plots/confusion_matrix.png\")\n",
        "        plt.show()\n",
        "\n",
        "    def predict_digit(self, image_tensor):\n",
        "        \"\"\"Predict the digit from a preprocessed image tensor.\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_tensor = image_tensor.unsqueeze(0).to(self.device)\n",
        "            output = self.model(image_tensor)\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "            probability, prediction = torch.max(probabilities, 1)\n",
        "\n",
        "        return prediction.item(), probability.item()\n",
        "\n",
        "    def visualize_prediction(self, index=0):\n",
        "        \"\"\"Visualize a prediction from the test set.\"\"\"\n",
        "        self.model.eval()\n",
        "        image, label = self.test_data[index]\n",
        "\n",
        "        prediction, probability = self.predict_digit(image)\n",
        "\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(image.squeeze(0), cmap='gray')\n",
        "        plt.title(f\"Prediction: {prediction} (Confidence: {probability:.2f})\\nActual: {label}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    def create_colab_drawing_interface(self):\n",
        "        \"\"\"Create an interactive drawing interface for Colab.\"\"\"\n",
        "        ColabDrawingInterface(self)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN1J1dSEM01x"
      },
      "source": [
        "# 5: Define the ColabDrawingInterface Class\n",
        "\n",
        "This new class replaces the Tkinter-based DrawingInterface with a Colab-compatible version using matplotlib for drawing and ipywidgets for buttons. Users can draw digits on a 280x280 canvas, and the image is resized to 28x28 for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YVM1xprYuByo"
      },
      "outputs": [],
      "source": [
        "class ColabDrawingInterface:\n",
        "    def __init__(self, digit_recognizer):\n",
        "        self.digit_recognizer = digit_recognizer\n",
        "        self.img = np.zeros((280, 280), dtype=np.float32)  # Black background\n",
        "\n",
        "        # Set up matplotlib figure\n",
        "        self.fig, self.ax = plt.subplots(figsize=(5, 5))\n",
        "        self.im = self.ax.imshow(self.img, cmap='gray', vmin=0, vmax=1)\n",
        "        self.ax.set_title('Draw a digit')\n",
        "        self.ax.axis('off')  # Hide axes for cleaner look\n",
        "\n",
        "        self.drawing = False\n",
        "\n",
        "        # Connect mouse events\n",
        "        self.cid_press = self.fig.canvas.mpl_connect('button_press_event', self.on_press)\n",
        "        self.cid_release = self.fig.canvas.mpl_connect('button_release_event', self.on_release)\n",
        "        self.cid_motion = self.fig.canvas.mpl_connect('motion_notify_event', self.on_motion)\n",
        "\n",
        "        # Create interactive buttons\n",
        "        self.predict_button = widgets.Button(description='Predict')\n",
        "        self.clear_button = widgets.Button(description='Clear')\n",
        "        self.predict_button.on_click(self.predict)\n",
        "        self.clear_button.on_click(self.clear)\n",
        "\n",
        "        # Display buttons\n",
        "        display(widgets.HBox([self.predict_button, self.clear_button]))\n",
        "        plt.show()\n",
        "\n",
        "    def on_press(self, event):\n",
        "        self.drawing = True\n",
        "\n",
        "    def on_release(self, event):\n",
        "        self.drawing = False\n",
        "\n",
        "    def on_motion(self, event):\n",
        "        if self.drawing and event.xdata is not None and event.ydata is not None:\n",
        "            x = int(event.xdata)\n",
        "            y = int(event.ydata)\n",
        "            # Simulate a brush by setting a 5x5 area to 1 (white)\n",
        "            for i in range(-2, 3):\n",
        "                for j in range(-2, 3):\n",
        "                    xi = x + i\n",
        "                    yj = y + j\n",
        "                    if 0 <= xi < 280 and 0 <= yj < 280:\n",
        "                        self.img[yj, xi] = 1\n",
        "            self.im.set_data(self.img)\n",
        "            self.fig.canvas.draw()\n",
        "\n",
        "    def clear(self, b):\n",
        "        self.img.fill(0)\n",
        "        self.im.set_data(self.img)\n",
        "        self.ax.set_title('Draw a digit')\n",
        "        self.fig.canvas.draw()\n",
        "\n",
        "    def predict(self, b):\n",
        "        # Convert numpy array to PIL image\n",
        "        pil_img = Image.fromarray((self.img * 255).astype(np.uint8))\n",
        "        # Resize to 28x28\n",
        "        small_pil = pil_img.resize((28, 28), Image.BILINEAR)\n",
        "        # Convert back to numpy and normalize to [0,1]\n",
        "        small_img = np.array(small_pil).astype(np.float32) / 255.0\n",
        "\n",
        "        # Apply MNIST normalization\n",
        "        transform = transforms.Normalize((0.1307,), (0.3081,))\n",
        "        image_tensor = torch.from_numpy(small_img).float().unsqueeze(0).unsqueeze(0)\n",
        "        image_tensor = transform(image_tensor)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction, confidence = self.digit_recognizer.predict_digit(image_tensor)\n",
        "\n",
        "        # Update title with prediction\n",
        "        self.ax.set_title(f'Prediction: {prediction} ({confidence*100:.2f}%)')\n",
        "        self.fig.canvas.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuY9CFzMM6OH"
      },
      "source": [
        "# 6: Define and Run the main Function\n",
        "\n",
        "The main function is updated to use the new Colab interface. We'll run it here to execute the entire program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f2b2ee0f9e3141a7805029ddd7499f69",
            "4781892e3f144923b66edf5a26ee7d66",
            "5be016e0d874407f8db1ef6d3e0266c1",
            "fb88d1767e444eec8e3dcd4e1bdeafc4",
            "ef9e8e183bb84935a1995fe8fa5cba12",
            "73e8adfa2b0d4bc5a81192375055b04f",
            "8b44f51bfbec4e90bc9f214dd3fa5902",
            "487f403f987b4af793d3cd719965d340"
          ]
        },
        "id": "S1eQvu5AvAOx",
        "outputId": "c67d3a03-d263-4dbd-93d4-5e1241a61fe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'transforms' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLaunching Drawing Interface...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     recognizer.create_colab_drawing_interface()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      4\u001b[39m     model_path = \u001b[33m\"\u001b[39m\u001b[33msaved_models/digit_model_best.pth\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(\u001b[33m\"\u001b[39m\u001b[33msaved_models/digit_model_best.pth\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     recognizer = \u001b[43mDigitRecognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      8\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo saved model found. Training a new model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mDigitRecognizer.__init__\u001b[39m\u001b[34m(self, model_path)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mself\u001b[39m.device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28mself\u001b[39m.train_transform = \u001b[43mtransforms\u001b[49m.Compose([\n\u001b[32m     11\u001b[39m     transforms.RandomRotation(\u001b[32m10\u001b[39m),\n\u001b[32m     12\u001b[39m     transforms.RandomAffine(degrees=\u001b[32m0\u001b[39m, translate=(\u001b[32m0.1\u001b[39m, \u001b[32m0.1\u001b[39m)),\n\u001b[32m     13\u001b[39m     transforms.ToTensor(),\n\u001b[32m     14\u001b[39m     transforms.Normalize((\u001b[32m0.1307\u001b[39m,), (\u001b[32m0.3081\u001b[39m,))\n\u001b[32m     15\u001b[39m ])\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.test_transform = transforms.Compose([\n\u001b[32m     18\u001b[39m     transforms.ToTensor(),\n\u001b[32m     19\u001b[39m     transforms.Normalize((\u001b[32m0.1307\u001b[39m,), (\u001b[32m0.3081\u001b[39m,))\n\u001b[32m     20\u001b[39m ])\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.create_model()\n",
            "\u001b[31mNameError\u001b[39m: name 'transforms' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def main():\n",
        "    model_path = \"saved_models/digit_model_best.pth\" if os.path.exists(\"saved_models/digit_model_best.pth\") else None\n",
        "    recognizer = DigitRecognizer(model_path)\n",
        "\n",
        "    if model_path is None:\n",
        "        print(\"No saved model found. Training a new model...\")\n",
        "        recognizer.prepare_data()\n",
        "        recognizer.train_model(epochs=5)  # Adjust epochs as needed\n",
        "    else:\n",
        "        print(f\"Using saved model: {model_path}\")\n",
        "        recognizer.prepare_data()\n",
        "\n",
        "    print(\"\\nVisualizing Predictions:\")\n",
        "    for i in range(5):\n",
        "        recognizer.visualize_prediction(i)\n",
        "\n",
        "    print(\"\\nLaunching Drawing Interface...\")\n",
        "    recognizer.create_colab_drawing_interface()\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "default",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4781892e3f144923b66edf5a26ee7d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Predict",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ef9e8e183bb84935a1995fe8fa5cba12",
            "style": "IPY_MODEL_73e8adfa2b0d4bc5a81192375055b04f",
            "tooltip": ""
          }
        },
        "487f403f987b4af793d3cd719965d340": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5be016e0d874407f8db1ef6d3e0266c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Clear",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8b44f51bfbec4e90bc9f214dd3fa5902",
            "style": "IPY_MODEL_487f403f987b4af793d3cd719965d340",
            "tooltip": ""
          }
        },
        "73e8adfa2b0d4bc5a81192375055b04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8b44f51bfbec4e90bc9f214dd3fa5902": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef9e8e183bb84935a1995fe8fa5cba12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2b2ee0f9e3141a7805029ddd7499f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4781892e3f144923b66edf5a26ee7d66",
              "IPY_MODEL_5be016e0d874407f8db1ef6d3e0266c1"
            ],
            "layout": "IPY_MODEL_fb88d1767e444eec8e3dcd4e1bdeafc4"
          }
        },
        "fb88d1767e444eec8e3dcd4e1bdeafc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
